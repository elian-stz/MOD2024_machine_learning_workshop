{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction au machine learning"
      ],
      "metadata": {
        "id": "28_ntdS-HpDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce workshop vise à présenter les bases du machine learning supervisé. A partir d'un jeu de données sur le cancer du sein, on va utiliser des algorithmes de classification pour prédire si des tumeurs sont malignes ou bénignes."
      ],
      "metadata": {
        "id": "9iOMRcrXHvdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliqué à notre jeu de données et de manière simplifiée, le but du machine learning supervisé est de donner des features (données d'entrée) à un modèle pour qu'il prédise une sortie (nature de la tumeur : maligne ou bénigne)."
      ],
      "metadata": {
        "id": "9mGEvIwsiizd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jupyter Notebook\n"
      ],
      "metadata": {
        "id": "UwR8nJuu5-70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passez cette partie si vous êtes déjà à l'aise avec les Jupyter Notebooks.**"
      ],
      "metadata": {
        "id": "5oirwxZGHmBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les Jupyter Notebooks sont des fichiers au format `.ipynb`. Ils proposent une interface qui allie code et prise de notes (comme RMarkdown avec R), ce qui est très pratique pour l'analyse de données ou le machine learning car on peut facilement revenir en arrière pour modifier le code sans devoir le réexécuter.\n",
        "\n",
        "## Environnements\n",
        "\n",
        "Les fichiers `.ipynb` nécessitent un environnement spécial pour être lus, édités et exécutés. Voici les 4 principaux :\n",
        "\n",
        "* Jupyter Notebook (plateforme web en local)\n",
        "* JupyterLab\n",
        "* Visual Studio Code (Codium sur les ordinateurs de la FdS)\n",
        "* Google Colab\n",
        "\n",
        "Nous allons travaillé avec Jupyter Notebook en instance web. Deux façons existent pour lancer un Jupyter Notebook :\n",
        "\n",
        "Via *interface graphique* : double cliquer sur le fichier `.ipynb` dans l'explorateur de fichiers.\n",
        "\n",
        "Via *lignes de commandes* : ouvrir un terminal, se placer dans le dossier contenant le notebook avec `cd`. Puis taper `jupyter notebook`, cela ouvre une interface web locale, puis sélectioner le notebook avec la souris.\n",
        "\n",
        "## Cellules\n",
        "\n",
        "Les cellules (chunks en anglais) sont des blocks qu'on peut exécuter. Il en existe deux sortes : les cellules de code Python et les cellules textuelles.\n",
        "\n",
        "Vous avez à disposition différents boutons pour modifier les cellules : déplacer une cellule en haut/bas, couper, supprimer, etc.\n",
        "\n",
        "**Les cellules peuvent être exécutées en sélectionnant d'abord la cellule d'intérêt avec la souris, puis soit en cliquant sur le bouton avec une flèche, soit avec le raccourci CTRL + entrée.**\n",
        "\n",
        "### Cellule de code\n",
        "\n",
        "Les cellules de code Python permettent d'exécuter directement du code Python.\n",
        "\n",
        "On peut préciser qu'une commande s'exécute en shell (bash sur systèmes Unix ou Powershell sur Windows) en utilisant `!` e.g. `!echo \"hello world\"`.\n",
        "\n",
        "### Cellule textuelle\n",
        "\n",
        "Les cellules textuelles permettent d'insérer du texte au format Markdown (cf. syntaxe Markdown) pour mettre des informations.\n",
        "\n",
        "## Sauvegarder les changements\n",
        "\n",
        "N'oubliez pas de sauvegarder les changements régulièrement avec le raccourci CTRL + S ou dans Fichier > Enregistrer au cas où ça plante.\n"
      ],
      "metadata": {
        "id": "awMNZq6f8DQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation des bibliothèques"
      ],
      "metadata": {
        "id": "FWANILVpYg88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On installe les bibliothèques (libraries) Python avec le gestionnaire officiel de bibliothèques Python, pip.\n",
        "\n",
        "* Scikit-Learn pour les modèles de machine learning\n",
        "* Matplotlib pour faire des graphes\n",
        "* Seaborn pour faire des graphes de façon plus simple\n",
        "* Pandas pour la manipulation de fichiers CSV (dataframes)"
      ],
      "metadata": {
        "id": "Ky-SaPvYIHar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYUR1pcI3riE"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Relancez le kernel après avoir installé les bibliothèques*** :\n",
        "\n",
        "(barre du haut) Kernel > Restart Kernel > Restart\n",
        "\n",
        "Si vous n'y arrivez pas, enregistrez le notebook, fermez la page et rouvrez le notebook."
      ],
      "metadata": {
        "id": "_n15efun8IY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation des bibliothèques"
      ],
      "metadata": {
        "id": "8ly5GQ_gYl7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On importe les bibliothèques et le jeu de données (dataset) du cancer du sein."
      ],
      "metadata": {
        "id": "kbJ4zWMeZ8Wk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS0vWqTS4IKX"
      },
      "outputs": [],
      "source": [
        "# On travaillera avec le dataset du cancer du sein\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Si ça ne marche pas, vous devez relancer le kernel :***\n",
        "\n",
        "(barre du haut) Kernel > Restart Kernel > Restart"
      ],
      "metadata": {
        "id": "P9L6aXtuJMrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation du contenu du jeu de données"
      ],
      "metadata": {
        "id": "3d4T0At9YqeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du jeu de données et première prise en main"
      ],
      "metadata": {
        "id": "z2l0PouinPNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de pré-traiter le jeu de données, une étape de visualisation est recommandée. Cela permet de mieux comprendre le jeu de données et de connaître les types de variables des features (quantitatives, qualitatives, etc.)"
      ],
      "metadata": {
        "id": "25JatIs2lrdA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hny_l_ni9qgT"
      },
      "source": [
        "Nous avons deux classes : B et M. Ces classes sont contenus dans la colonne **target** (d'abord binaire). Chaque classe représente une classification de cancer, les codes sont les suivants :\n",
        "\n",
        "* 1 = \"bening\"  \n",
        "* 0 = \"malignant\"\n",
        "\n",
        "On extrait :\n",
        "* les features qui vont nous permettre de classifier les instances\n",
        "* les labels qui contiennent les colonnes d'indice et de classe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3erq3PC4U-9"
      },
      "outputs": [],
      "source": [
        "# Charge les données sous forme de tuple\n",
        "features, labels = load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Concatène les features et labels ensemble\n",
        "dataset = pd.concat([features, labels], axis=1)\n",
        "\n",
        "# Affiche les premières lignes du dataset\n",
        "display(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On transforme la colonne `target` (0, 1) en `target_names` contenant des chaînes de caractères (\"malignant\", \"benign\")"
      ],
      "metadata": {
        "id": "nDCT3m8SQ9W3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFKfWnSBHY-D"
      },
      "outputs": [],
      "source": [
        "# Charge le dataset de cancer du sein à partir de la bibliothèque Scikit-Learn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Crée un dataframe Pandas à partir des données du dataset avec les noms de colonnes correspondant aux features\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Ajoute une colonne 'target_names' au dataframe et remplace les données binaires\n",
        "df['target_names'] = data.target_names[data.target]\n",
        "\n",
        "# Affiche les premières lignes du dataframe\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On liste toutes les features."
      ],
      "metadata": {
        "id": "V-2tLk0EtOq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Affiche tous les noms de colonnes présents dans le dataset\n",
        "display(dataset.columns)\n",
        "\n",
        "# Crée une liste des colonnes contenant le mot \"mean\"\n",
        "mean_columns = [col for col in dataset.columns if 'mean' in col]\n",
        "\n",
        "# Crée une liste des colonnes contenant le mot \"error\"\n",
        "error_columns = [col for col in dataset.columns if 'error' in col]\n",
        "\n",
        "# Crée une liste des colonnes contenant le mot \"worst\"\n",
        "worst_columns = [col for col in dataset.columns if 'worst' in col]\n",
        "\n",
        "# Affiche les colonnes contenant le mot \"mean\"\n",
        "display(mean_columns)\n",
        "\n",
        "# Affiche les colonnes contenant le mot \"error\"\n",
        "display(error_columns)\n",
        "\n",
        "# Affiche les colonnes contenant le mot \"worst\"\n",
        "display(worst_columns)"
      ],
      "metadata": {
        "id": "MzOG0A1DXjRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a 10 features : radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension\n",
        "\n",
        "Chaque feature a :\n",
        "* la mesure moyenne (mean)\n",
        "* l'écart-type des valeurs (error)\n",
        "* la moyenne des 3 valeurs les plus élevées (worst)"
      ],
      "metadata": {
        "id": "JAorOI2FySn9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IHTBKVE48p1"
      },
      "outputs": [],
      "source": [
        "def data_info(dataset: pd.DataFrame) -> None :\n",
        "    \"\"\"\n",
        "    Résume de façon succincte le contenu du jeu de données\n",
        "    \"\"\"\n",
        "    # Infos sur les dimensions du dataset\n",
        "    print(\"DIMENSIONS :\\n\")\n",
        "    dims = dataset.shape\n",
        "    print(\"\\tLe dataset contient {} instances (observations), et {} features.\\n\".format(dims[0], dims[1]))\n",
        "    print(\"\\tLes features sont : {}.\\n\".format(dataset.columns.tolist()))\n",
        "\n",
        "    # Infos sur les classes\n",
        "    print(\"CLASSES :\\n\")\n",
        "    classes = df[\"target_names\"].value_counts().index.tolist()\n",
        "    print(\"\\tIl y a {} classes dans le dataset.\\n\".format(len(classes)))\n",
        "    print(\"\\tLes classes sont : {}.\\n\".format(classes))\n",
        "\n",
        "    # Infos de duplications\n",
        "    print(\"DUPLICATION :\\n\")\n",
        "    subset_columns = list(dataset.columns)\n",
        "    # Verification de duplication dans les colonnes du sous-ensemble\n",
        "    duplicated_rows = dataset.duplicated(subset=subset_columns, keep=False)\n",
        "    # Comptage de lignes dupliquées\n",
        "    duplicated_count = duplicated_rows.sum()\n",
        "    print(\"\\tIl y a {} lignes dupliquées dans le dataset.\\n\".format(duplicated_count))\n",
        "    # Affiche le nombre de lignes dupliquées\n",
        "    if duplicated_count > 0:\n",
        "      rows_duplicated = dataset[duplicated_rows]\n",
        "      print(\"\\tLes lignes dupliquées sont :\\n\")\n",
        "      print(rows_duplicated)\n",
        "    else:\n",
        "      print(\"\\tAucune ligne dupliquée dans le dataset.\\n\")\n",
        "\n",
        "data_info(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistiques globales des features"
      ],
      "metadata": {
        "id": "SdYGhZTqm_Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A présent, on regarde les statistiques par classe.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xtYEwSjt1bT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI2bdSZC67IZ"
      },
      "outputs": [],
      "source": [
        "def stats_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Statistiques globales du dataset\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset.iloc[:, 0:30].describe())\n",
        "\n",
        "    # Statistiques par classe\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist() :\n",
        "\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i].iloc[:, 0:30].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On observe que les features de la classe bénigne (1) ont tendance à être inférieures à celles de la classe maligne (0).\n",
        "\n",
        "Exemple :\n",
        "* La moyenne de `mean radius` est de 12.146524 pour la classe bénigne (0) mais de 17.462830 pour la classe maligne (1)\n",
        "* La médiane de `mean texture` est de 17.390000 pour la classe bénigne (0) et de 21.460000 pour la classe maligne (1)"
      ],
      "metadata": {
        "id": "6vzdQiwdwldk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le tableau étant partiellement tronqué, on regarde ici que les features contenant `error`, ce qui correspond à l'écart-type."
      ],
      "metadata": {
        "id": "_etzUQMAx-WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Affiche spécifiquement les statistiques globales pour les colonnes error\n",
        "    \"\"\"\n",
        "    # Statistiques globales pour les colonnes contenant \"error\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[error_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"error\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][error_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "8xc-WqERxKi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 1"
      ],
      "metadata": {
        "id": "fYl3UIpywVvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sélectionnez le groupe des features le plus pertinent entre `error`, `mean`, `worst`. Ensuite affichez les statistiques globales et par classe du groupe que vous avez choisi.\n",
        "\n",
        "Utilisez les variables `mean_columns`, `error_columns` ou `worst_columns` pour cela."
      ],
      "metadata": {
        "id": "pjvnrbDKjlb1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bz-HCRP5XzGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box plots des features"
      ],
      "metadata": {
        "id": "HnY3Fbogy0ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une manière plus visuelle de montrer la distribution du jeu de données est d'utiliser des box plots (boîtes à moustache) qui permettent d'observer des tendances entre les classes.\n",
        "\n",
        "Ci-dessous on regarde seulement les features avec `error` :"
      ],
      "metadata": {
        "id": "HmkIs4LemYxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boxplot_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Récupérer les classes\n",
        "    classes = dataset[\"target_names\"].value_counts().index.tolist()\n",
        "\n",
        "    # Récupérer le nom des variables\n",
        "    features_names = [col_name for col_name in dataset.columns.tolist() if \"error\" in col_name and col_name != \"target\"]\n",
        "\n",
        "    # Définir le compteur des subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Définir une figure\n",
        "    plt.figure(figsize=(20, 40))\n",
        "\n",
        "    # Pour chaque variable\n",
        "    for col_name in features_names :\n",
        "        # Sur un sous-plot\n",
        "        plt.subplot(len(features_names), len(classes), cpt)\n",
        "\n",
        "        # Afficher le box plot de la distribution des valeurs de la variable en fonction des classes\n",
        "        sns.boxplot(data=dataset, x=\"target_names\", y=col_name, hue=\"target_names\")\n",
        "\n",
        "        # Passer au subbplot suivant\n",
        "        cpt+=1\n",
        "\n",
        "    # Ajuster l'agencement\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Afficher la figure\n",
        "    plt.show()\n",
        "\n",
        "boxplot_data(df)"
      ],
      "metadata": {
        "id": "4aJng7199UI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 2"
      ],
      "metadata": {
        "id": "aJFqbvBs9WQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faites pareil que ci-dessus mais en sélectionnant les groupes de features correspondant à `mean` et/ou à `worst` pour visualiser les box plots associés."
      ],
      "metadata": {
        "id": "SQ_u8OtS0PrY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8AcbivT11Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces box plots confirment nos précédentes observations : les instances de la classe maligne ont tendance à avoir des features moins élevées que les instances de la classe bénigne."
      ],
      "metadata": {
        "id": "-OJVIfRWmEfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scatter plots des features"
      ],
      "metadata": {
        "id": "a3JUvVDY9KSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces scatter plots vont permettre de montrer des relations par paire entre les variables de données. Chaque sous-graphe de la grille représente la relation entre deux variables différentes."
      ],
      "metadata": {
        "id": "BMO9U9MQlkoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsuZWQO2Llwm"
      },
      "outputs": [],
      "source": [
        "def features_distributions(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Récupérer le nom des variables\n",
        "    features_names = [\"mean perimeter\", \"mean texture\", \"mean area\", \"mean radius\"]\n",
        "\n",
        "    # Récupérer le nombre de variables\n",
        "    n_features = len(features_names)\n",
        "\n",
        "    # Définir le compteur des subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Définir une figure\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    # Afficher chaque variable en fonction des autres\n",
        "    for i in features_names :\n",
        "\n",
        "        for j in features_names :\n",
        "\n",
        "            # Sur un subplot\n",
        "            plt.subplot(n_features, n_features, cpt)\n",
        "\n",
        "            # Afficher la distribution des espèces dans le plan\n",
        "            plt.scatter(dataset.loc[:, i], dataset.loc[:, j], c=dataset[\"target\"])\n",
        "\n",
        "            # Nommer les axes\n",
        "            plt.ylabel(i)\n",
        "            plt.xlabel(j)\n",
        "\n",
        "            # Passer au suivant\n",
        "            cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Afficher la figure\n",
        "    plt.show()\n",
        "\n",
        "features_distributions(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de ces scatter plots, on peut voir que les instances de même classe (= même couleur) ont tendance à se regrouper. Cela est bon signe pour une approche de classification."
      ],
      "metadata": {
        "id": "nCNMWEQIlHEW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jdAngBUTw_M"
      },
      "source": [
        "# Prétraitement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'étape de prétraitement du jeu de données permet de le préparer à l'entraînement car les modèles sont sensibles à certaines caractéristiques : données manquantes ou dupliquées, outliers, etc. Ainsi, cette étape vise à homogéneiser le jeu de données.\n",
        "\n",
        "Cette étape de prétraitement contient plusieurs sous-étapes :\n",
        "* **Nettoyage** : retrait des données dupliquées/manquantes\n",
        "* **Normalisation** : homogénéiser les données\n",
        "\n",
        "> D'autres sous-étapes existent comme l'**extraction de features** (non présentée ici) qui consiste à extraire les données. Par exemple, pour analyser un texte on va en extraire des mots sémantiquement riches (tokens) et les transformer en données quantitatives. Ainsi, on transforme l'information textuelle en nombres.\n",
        "\n",
        "> Une autre sous-étape est l'**équilibrage** des classes. Elle n'a pas été faite ici : certains algorithmes sont peu sensibles aux classes déséquilibrées.\n",
        "\n",
        "> En lien avec l'équilibrage des classes, une sous-étape de **resampling** peut-être nécessaire : elle consiste à rééchantillonner le jeu de données pour le rendre plus équilibré. Le resampling inclut le downsampling, l'upsampling, et l'interpolation.\n",
        "\n",
        "Enfin, la dernière étape est le **découpage** du jeu de données : faire un jeu de données d'entraînement et un autre pour la validation."
      ],
      "metadata": {
        "id": "qEMKlb9HFgnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOaq7ZkjTyRL"
      },
      "outputs": [],
      "source": [
        "# La fonction MinMaxScaler permet de normaliser les données\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCedoZP5T2L9"
      },
      "outputs": [],
      "source": [
        "# Retire les lignes dupliquées du dataset\n",
        "dataset = dataset[dataset.duplicated() == False].reset_index(drop=True)\n",
        "\n",
        "# Affiche quelques infos\n",
        "display(dataset.head())\n",
        "print(\"dimensions : {}\".format(dataset.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLc8EhjtT6E8"
      },
      "outputs": [],
      "source": [
        "# Resplitte le dataset en deux\n",
        "features, labels, target_names = dataset.iloc[:, 0:30], dataset.iloc[:, 30], df[\"target_names\"]\n",
        "\n",
        "# Crée un objet MinMaxScaler\n",
        "normalizer = MinMaxScaler()\n",
        "\n",
        "# Normalise les variables\n",
        "features = normalizer.fit_transform(features)\n",
        "\n",
        "# Retransforme les features en dataframe\n",
        "features = pd.DataFrame(data=features, columns=normalizer.get_feature_names_out())\n",
        "\n",
        "# Affiche les résultats de la normalisation\n",
        "display(features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le jeu de données étant relativement propre, on n'a pas besoin de le prétraiter plus que ça. On peut passer à l'entraînement."
      ],
      "metadata": {
        "id": "WbUiBGJYJj-F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwBiZwq6UFK5"
      },
      "source": [
        "# Entraînement et évaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme son nom l'indique, l'étape d'entraînement consiste à entraîner un modèle, c'est-à-dire que le modèle va ajuster ses paramètres/poids pour prédire au mieux la sortie : la classe (maligne ou bénigne).\n",
        "\n",
        "On va utiliser 6 algorithmes de classification\n",
        "* Naive Bayes\n",
        "* Régression logistique\n",
        "* Machine à vecteur de support (support-vector machine)\n",
        "* Forêt d'arbre décisionnels (random forest)\n",
        "* XGboost\n",
        "* Perceptron multicouche (multilayer perceptron)"
      ],
      "metadata": {
        "id": "CKZgD2jIKKLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDPNJiXHUF5A"
      },
      "outputs": [],
      "source": [
        "# Permet de de splitter les données en un set de données d'entraînement, et un set de données de test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Plusieurs algorithmes d'apprentissage que nous allons tester\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# Permet l'évaluation des modèles\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Permet la recherche des meilleurs hyperparamètres\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxXo3eTBUJBi"
      },
      "outputs": [],
      "source": [
        "# Séparer les données en quatres sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "# Afficher les sets\n",
        "## Entrée\n",
        "display(x_train.head())\n",
        "display(x_test.head())\n",
        "## Sortie : classe à prédire\n",
        "display(y_train.head())\n",
        "display(y_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le jeu de données d'entraînement correspond à 80% de la taille originelle du jeu de données. Le jeu de données de test correspond donc à 20% de sa taille.\n",
        "\n",
        "Le découpage se fait aléatoirement mais on peut \"fixer le niveau d'aléatoire\" pour la reproductibilité avec le paramètre `random_state`."
      ],
      "metadata": {
        "id": "oAWewGaPNLv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va évaluer les performances des modèles avec une matrice de confusion. Une matrice de confusion consiste à comparer les classes prédites par le modèle aux classes réelles (vrai positif, faux négatif, etc.). A partir de cette matrice, on peut calculer les métriques de performance :\n",
        "* Precision : plus elle est élevée et plus on maximise les vrais positifs\n",
        "* Recall : plus il est élevé et plus on maximise les vrais (vrai positif et vrai négatif)\n",
        "* Accuracy : plus elle est élevée et plus on maximise les bonnes prédictions (faux négatif et vrai positif)\n",
        "* F1-score : combine la precision et le recall (moyenne harmonique)\n",
        "\n",
        "Toutes ces métriques sont importantes mais c'est surtout la **precision** et le **recall** (= rappel = sensibilité) qui nous intéressent étant donné que si on favorise l'un, l'autre décroît. Ainsi, selon le jeu de données et l'objectif de la classification on préfèrera soit favoriser l'un au détriment de l'autre (trade-off).\n",
        "\n",
        "Pour la prédiction de cancer du sein, on préfèrera détecter davantage de vrais positifs : si on détecte une tumeur maligne alors elle a de forte chance de l'être. Ainsi, on aura tendance à favoriser la précision du fait que les traitements soient lourds. Mais au final cela reste un choix."
      ],
      "metadata": {
        "id": "en_FyBbPTdzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flhb5Jnf_piM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Définir une liste d'algorithmes d'apprentissage\n",
        "learning_algo = [MultinomialNB(),\n",
        "                 LogisticRegression(random_state=42),\n",
        "                 SVC(random_state=42),\n",
        "                 DecisionTreeClassifier(random_state=42),\n",
        "                 GradientBoostingClassifier(random_state=42),\n",
        "                 MLPClassifier(random_state=42, max_iter=1000)\n",
        "                ]\n",
        "\n",
        "# Initialiser des variables pour suivre les meilleurs modèles et leurs métriques de performance\n",
        "max_accuracy = 0\n",
        "max_precision = 0\n",
        "max_recall = 0\n",
        "best_model_accuracy = None\n",
        "best_model_precision = None\n",
        "best_model_recall = None\n",
        "\n",
        "# Parcourir chaque algorithme d'apprentissage\n",
        "for algo in learning_algo:\n",
        "    tmp = algo  # Assigner l'algorithme actuel à la variable tmp\n",
        "    print(type(tmp).__name__ + \":\")  # Afficher le nom de l'algorithme actuel\n",
        "\n",
        "    # Entraîner le modèle sur les données d'entraînement\n",
        "    tmp.fit(x_train, y_train)\n",
        "\n",
        "    # Créer une figure pour l'affichage de la matrice de confusion\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Afficher la matrice de confusion pour l'ensemble d'entraînement\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_train, y_train, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 1))\n",
        "    plt.title(\"Matrice de confusion - Ensemble d'entraînement\")\n",
        "\n",
        "    # Afficher la matrice de confusion pour l'ensemble de test\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_test, y_test, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 2))\n",
        "    plt.title(\"Matrice de confusion - Ensemble de test\")\n",
        "\n",
        "    plt.show()  # Afficher les graphiques des matrices de confusion\n",
        "\n",
        "    # Faire des prédictions sur les ensembles d'entraînement et de test\n",
        "    y_pred_train = tmp.predict(x_train)\n",
        "    y_pred_test = tmp.predict(x_test)\n",
        "\n",
        "    # Calculer les métriques de performance sur l'ensemble d'entraînement\n",
        "    accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
        "    precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
        "    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
        "\n",
        "    # Calculer les métriques de performance sur l'ensemble de test\n",
        "    accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
        "    precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Afficher le rapport de classification pour l'ensemble d'entraînement\n",
        "    print(\"\\nENSEMBLE D'ENTRAÎNEMENT :\\n\")\n",
        "    print(classification_report(y_pred_train, y_train))\n",
        "\n",
        "    # Afficher les métriques de performance pour l'ensemble d'entraînement\n",
        "    print(\"\\nAccuracy sur l'ensemble d'entraînement:\", accuracy_train)\n",
        "    print(\"Precision sur l'ensemble d'entraînement:\", precision_train)\n",
        "    print(\"recall sur l'ensemble d'entraînement:\", recall_train)\n",
        "\n",
        "    # Afficher le rapport de classification pour l'ensemble de test\n",
        "    print(\"\\nENSEMBLE DE TEST :\\n\")\n",
        "    print(classification_report(y_pred_test, y_test))\n",
        "\n",
        "    # Afficher les métriques de performance pour l'ensemble de test\n",
        "    print(\"Accuracy sur l'ensemble de test:\", accuracy_test)\n",
        "    print(\"Precision sur l'ensemble de test:\", precision_test)\n",
        "    print(\"recall sur l'ensemble de test:\", recall_test)\n",
        "\n",
        "    # Mettre à jour les valeurs maximales et le meilleur modèle si le modèle actuel est meilleur\n",
        "    if accuracy_test > max_accuracy:\n",
        "        max_accuracy = accuracy_test\n",
        "        best_model_accuracy = type(tmp).__name__\n",
        "\n",
        "    if precision_test > max_precision:\n",
        "        max_precision = precision_test\n",
        "        best_model_precision = type(tmp).__name__\n",
        "\n",
        "    if recall_test > max_recall:\n",
        "        max_recall = recall_test\n",
        "        best_model_recall = type(tmp).__name__\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "# Afficher les meilleurs modèles et leurs métriques de performance\n",
        "print(f\"Meilleur modèle (Accuracy): {best_model_accuracy} - Accuracy: {max_accuracy:.4f}\")\n",
        "print(f\"Meilleur modèle (Precision): {best_model_precision} - Precision: {max_precision:.4f}\")\n",
        "print(f\"Meilleur modèle (recall): {best_model_recall} - recall: {max_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par la suite on va rechercher les meilleurs hyperparamètres, c'est-à-dire ceux qui vont donner la meilleure prédiction. Les hyperparamètres sont les paramètres qui sont propres aux algorithmes et vont influer sur leur comportement.\n",
        "\n",
        "Ici on s'intéresse à optimiser les hyperparamètres pour tous les modèles :"
      ],
      "metadata": {
        "id": "DhyUtp05fEkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHjoik9mUnkZ"
      },
      "outputs": [],
      "source": [
        "# Instancier un svm\n",
        "model = SVC()\n",
        "\n",
        "# Définir les paramètres à tester\n",
        "params = {\"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "          \"random_state\" : [i for i in range(0, 100, 1)]}\n",
        "\n",
        "# Instancier l'itérateur pour la création du set de validation durant la recherche des meilleurs hyperparamètres\n",
        "kfold = 10\n",
        "\n",
        "# Créer la grille de recherche\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", cv=kfold, verbose=3)\n",
        "\n",
        "# Effectuer la recherche + Afficher les résultats\n",
        "grid_search.fit(x_train, y_train)\n",
        "print(\"\\nles meilleurs paramètres du modèle sont : {}\".format(grid_search.best_params_))\n",
        "\n",
        "#les meilleurs paramètres du modèle sont : {'C': 10, 'random_state': 0} avec kfold= LeaveOneOut() et kfold= 10, pour économiser de temps on utilise kfold= 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPXTVCpqUrjU"
      },
      "outputs": [],
      "source": [
        "# Récupérer le meilleur modèle\n",
        "model = grid_search.best_estimator_\n",
        "print(model)\n",
        "# Créer une figure\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Afficher la matrice de confusion de model sur les données d'entraînement\n",
        "ConfusionMatrixDisplay.from_estimator(model, x_train, y_train, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 1))\n",
        "plt.title(\"Matrice de confusion - Ensemble d'entraînement\")\n",
        "\n",
        "# Afficher la matrice de confusion de model sur les données de test\n",
        "ConfusionMatrixDisplay.from_estimator(model, x_test, y_test, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 2))\n",
        "plt.title(\"Matrice de confusion - Ensemble de test\")\n",
        "\n",
        "# Afficher la figure\n",
        "plt.show()\n",
        "\n",
        "# Afficher le rapport de classification sur les données d'entraînement\n",
        "print(\"\\nENSEMBLE D'ENTRAINEMENT:\\n\")\n",
        "print(classification_report(model.predict(x_train), y_train))\n",
        "\n",
        "# Afficher le rapport de classification sur les données de test\n",
        "print(\"\\nENSEMBLE DE TEST :\\n\")\n",
        "print(classification_report(model.predict(x_test), y_test))\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLVDjSEKUvyO"
      },
      "source": [
        "# Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On sauvegarde le modèle pour ensuite l'appliquer sur d'autres jeux de données.\n",
        "\n",
        "Le modèle est enregistré au format `.sav` qui est ici binaire."
      ],
      "metadata": {
        "id": "jKmQ0tmzQSUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQE7NpT_UxDP"
      },
      "outputs": [],
      "source": [
        "# Permet de sauvegarder des objets Python\n",
        "import pickle as pk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtMDunldUzem"
      },
      "outputs": [],
      "source": [
        "# Instancier le modèle avec les meilleurs hyperparamètres\n",
        "model = SVC(C=10, random_state=0)\n",
        "\n",
        "# Entraîner le modèle sur l'ensemble des données\n",
        "model.fit(features, labels)\n",
        "\n",
        "# Afficher la matrice de confusion\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.title(\"Matrice de confusion finale\")\n",
        "ConfusionMatrixDisplay.from_estimator(model, features, labels, cmap=plt.cm.Blues, ax=plt.subplot(1, 1, 1))\n",
        "plt.show()\n",
        "\n",
        "# Afficher le rapport de classification\n",
        "print(classification_report(model.predict(features), labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoILiz4aU08g"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder le préprocesseur, il sera utiliser sur les futurs données\n",
        "pk.dump(normalizer, open(\"preprocesseur.sav\", 'wb'))\n",
        "\n",
        "# Sauvegarder le modèle\n",
        "pk.dump(model, open(\"model.sav\", 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice synthétique"
      ],
      "metadata": {
        "id": "XT8PTRUJrSxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de ce que vous avez vu, entraînez un modèle sur un mini jeu de données (voir cellule ci-dessous). Identifiez les fonctions importantes et entraîner un ou plusieurs modèle(s) parmi les 6 vus précédemment.\n",
        "\n",
        "Dans ce mini jeu de données, on doit prédire la classe `y` (chien ou chat) à partir de `x` (contenant 2 features : taille en cm et poids en kg). On a 6 instances."
      ],
      "metadata": {
        "id": "o3ep7f5zrYx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[45, 11.34], [56, 13.61], [51, 12.7], [30, 4.5], [25, 3.8], [35, 5.2]] # features\n",
        "y = [\"chien\", \"chien\", \"chien\", \"chat\", \"chat\", \"chat\"] # classe"
      ],
      "metadata": {
        "id": "p_kS1cjksCK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corrections"
      ],
      "metadata": {
        "id": "QPn5x7yHoj5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 1"
      ],
      "metadata": {
        "id": "jbfuNdGboozU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour la colonne worst\n",
        "\n",
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    # Statistiques globales pour les colonnes contenant \"worst\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[worst_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"worst\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][worst_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "xg95TxLFYQEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour la colonne error\n",
        "\n",
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    # Statistiques globales pour les colonnes contenant \"error\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[error_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"error\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][error_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "9FHT3hhho1wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour la colonne mean\n",
        "\n",
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    # Statistiques globales pour les colonnes contenant \"mean\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[mean_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"mean\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][mean_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "fd6_8G9PpCmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 2"
      ],
      "metadata": {
        "id": "PdfGQoJ4qV-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pour worst\n",
        "\n",
        "def boxplot_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Récupèrer les classes\n",
        "    classes = dataset[\"target_names\"].value_counts().index.tolist()\n",
        "\n",
        "    # Récupèrer le nom des variables\n",
        "    features_names = [col_name for col_name in dataset.columns.tolist() if \"worst\" in col_name and col_name != \"target\"] # Solution: changer le contenu de \"\" après le if\n",
        "\n",
        "    # Définir le compteur des subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Définir une figure\n",
        "    plt.figure(figsize=(20, 40))\n",
        "\n",
        "    # Pour chaque variables\n",
        "    for col_name in features_names :\n",
        "        # Sur un suplot\n",
        "        plt.subplot(len(features_names), len(classes), cpt)\n",
        "\n",
        "        # Afficher le boxplot de la distribution des valeurs de la variable, en fonction des espèces\n",
        "        sns.boxplot(data=dataset, x=\"target_names\", y=col_name, hue=\"target_names\")\n",
        "\n",
        "        # Passer au subplot suivant\n",
        "        cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Afficher la figure\n",
        "    plt.show()\n",
        "\n",
        "boxplot_data(df)"
      ],
      "metadata": {
        "id": "AY1MGe0aYeGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pour mean\n",
        "\n",
        "def boxplot_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Récupèrer les classes\n",
        "    classes = dataset[\"target_names\"].value_counts().index.tolist()\n",
        "\n",
        "    # Récupèrer le nom des variables\n",
        "    features_names = [col_name for col_name in dataset.columns.tolist() if \"mean\" in col_name and col_name != \"target\"] # Solution: changer le contenu de \"\" après le if\n",
        "\n",
        "    # Définir le compteur des subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Définir une figure\n",
        "    plt.figure(figsize=(20, 40))\n",
        "\n",
        "    # Pour chaque variables\n",
        "    for col_name in features_names :\n",
        "        # Sur un suplot\n",
        "        plt.subplot(len(features_names), len(classes), cpt)\n",
        "\n",
        "        # Afficher le boxplot de la distribution des valeurs de la variable, en fonction des espèces\n",
        "        sns.boxplot(data=dataset, x=\"target_names\", y=col_name, hue=\"target_names\")\n",
        "\n",
        "        # Passer au subplot suivant\n",
        "        cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Afficher la figure\n",
        "    plt.show()\n",
        "\n",
        "boxplot_data(df)"
      ],
      "metadata": {
        "id": "Gj0l3xq-1w05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice synthétique"
      ],
      "metadata": {
        "id": "1gjDAZZ-u52T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[45, 11.34], [56, 13.61], [51, 12.7], [30, 4.5], [25, 3.8], [35, 5.2]] # features\n",
        "y = [\"chien\", \"chien\", \"chien\", \"chat\", \"chat\", \"chat\"] # classe\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, stratify=y, random_state=42)\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Définir une liste d'algorithmes d'apprentissage\n",
        "learning_algo = [MultinomialNB(),\n",
        "                 LogisticRegression(random_state=42),\n",
        "                 SVC(random_state=42),\n",
        "                 DecisionTreeClassifier(random_state=42),\n",
        "                 GradientBoostingClassifier(random_state=42),\n",
        "                 MLPClassifier(random_state=42, max_iter=1000)\n",
        "                ]\n",
        "\n",
        "# Initialiser des variables pour suivre les meilleurs modèles et leurs métriques de performance\n",
        "max_accuracy = 0\n",
        "max_precision = 0\n",
        "max_recall = 0\n",
        "best_model_accuracy = None\n",
        "best_model_precision = None\n",
        "best_model_recall = None\n",
        "\n",
        "# Parcourir chaque algorithme d'apprentissage\n",
        "for algo in learning_algo:\n",
        "    tmp = algo  # Assigner l'algorithme actuel à la variable tmp\n",
        "    print(type(tmp).__name__ + \":\")  # Afficher le nom de l'algorithme actuel\n",
        "\n",
        "    # Entraîner le modèle sur les données d'entraînement\n",
        "    tmp.fit(x_train, y_train)\n",
        "\n",
        "    # Créer une figure pour l'affichage de la matrice de confusion\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Afficher la matrice de confusion pour l'ensemble d'entraînement\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_train, y_train, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 1))\n",
        "    plt.title(\"Matrice de confusion - Ensemble d'entraînement\")\n",
        "\n",
        "    # Afficher la matrice de confusion pour l'ensemble de test\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_test, y_test, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 2))\n",
        "    plt.title(\"Matrice de confusion - Ensemble de test\")\n",
        "\n",
        "    plt.show()  # Afficher les graphiques des matrices de confusion\n",
        "\n",
        "    # Faire des prédictions sur les ensembles d'entraînement et de test\n",
        "    y_pred_train = tmp.predict(x_train)\n",
        "    y_pred_test = tmp.predict(x_test)\n",
        "\n",
        "    # Calculer les métriques de performance sur l'ensemble d'entraînement\n",
        "    accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
        "    precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
        "    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
        "\n",
        "    # Calculer les métriques de performance sur l'ensemble de test\n",
        "    accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
        "    precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Afficher le rapport de classification pour l'ensemble d'entraînement\n",
        "    print(\"\\nENSEMBLE D'ENTRAÎNEMENT :\\n\")\n",
        "    print(classification_report(y_pred_train, y_train))\n",
        "\n",
        "    # Afficher les métriques de performance pour l'ensemble d'entraînement\n",
        "    print(\"\\nAccuracy sur l'ensemble d'entraînement:\", accuracy_train)\n",
        "    print(\"Precision sur l'ensemble d'entraînement:\", precision_train)\n",
        "    print(\"recall sur l'ensemble d'entraînement:\", recall_train)\n",
        "\n",
        "    # Afficher le rapport de classification pour l'ensemble de test\n",
        "    print(\"\\nENSEMBLE DE TEST :\\n\")\n",
        "    print(classification_report(y_pred_test, y_test))\n",
        "\n",
        "    # Afficher les métriques de performance pour l'ensemble de test\n",
        "    print(\"Précision sur l'ensemble de test:\", accuracy_test)\n",
        "    print(\"Precision sur l'ensemble de test:\", precision_test)\n",
        "    print(\"recall sur l'ensemble de test:\", recall_test)\n",
        "\n",
        "    # Mettre à jour les valeurs maximales et le meilleur modèle si le modèle actuel est meilleur\n",
        "    if accuracy_test > max_accuracy:\n",
        "        max_accuracy = accuracy_test\n",
        "        best_model_accuracy = type(tmp).__name__\n",
        "\n",
        "    if precision_test > max_precision:\n",
        "        max_precision = precision_test\n",
        "        best_model_precision = type(tmp).__name__\n",
        "\n",
        "    if recall_test > max_recall:\n",
        "        max_recall = recall_test\n",
        "        best_model_recall = type(tmp).__name__\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "# Afficher les meilleurs modèles et leurs métriques de performance\n",
        "print(f\"Meilleur modèle (Précision): {best_model_accuracy} - Précision: {max_accuracy:.4f}\")\n",
        "print(f\"Meilleur modèle (Precision): {best_model_precision} - Precision: {max_precision:.4f}\")\n",
        "print(f\"Meilleur modèle (recall): {best_model_recall} - recall: {max_recall:.4f}\")"
      ],
      "metadata": {
        "id": "coOjdSVCu9MI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}