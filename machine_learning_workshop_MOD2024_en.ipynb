{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning 101"
      ],
      "metadata": {
        "id": "28_ntdS-HpDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This workshop aims to introduce the basics of supervised machine learning algorithms. Using a breast cancer dataset, we will employ classification algorithms to predict whether tumors are malignant or benign."
      ],
      "metadata": {
        "id": "9iOMRcrXHvdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied to our dataset and in a simplified manner, the goal of supervised machine learning is to provide features (input data) to a model so that it predicts an output (nature of the tumor: malignant or benign)."
      ],
      "metadata": {
        "id": "9mGEvIwsiizd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jupyter Notebook\n"
      ],
      "metadata": {
        "id": "UwR8nJuu5-70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip this section if you know how to use Jupyter Notebooks**\n"
      ],
      "metadata": {
        "id": "5oirwxZGHmBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jupyter Notebooks are files in the `.ipynb` format. They provide an interface that combines code and note-taking (similar to RMarkdown with R), which is very convenient for data analysis or machine learning because you can easily go back to modify the code without having to rerun it.\n",
        "\n",
        "## Environments\n",
        "\n",
        "`.ipynb` files require a special environment to be read, edited, and executed. Here are the 4 main ones:\n",
        "\n",
        "* Jupyter Notebook (local web platform)\n",
        "* JupyterLab\n",
        "* Visual Studio Code (Codium on the campus' computers)\n",
        "* Google Colab\n",
        "\n",
        "We will work with Jupyter Notebook in a web instance. There are two ways to launch a Jupyter Notebook:\n",
        "\n",
        "Via *graphical interface*: double-click on the `.ipynb` file in the file explorer.\n",
        "\n",
        "Via *command lines*: open a terminal, navigate to the folder containing the notebook with `cd`. Then type `jupyter notebook`, which opens a local web interface, and select the notebook with the mouse.\n",
        "\n",
        "## Cells\n",
        "\n",
        "Cells are blocks that can be executed. There are two types: Python code cells and text cells.\n",
        "\n",
        "You have various buttons to modify the cells: move a cell up/down, cut, delete, etc.\n",
        "\n",
        "**Cells can be executed by first selecting the cell of interest with the mouse, then either clicking the button with an arrow or using the shortcut CTRL + Enter.**\n",
        "\n",
        "### Code cell\n",
        "\n",
        "Python code cells allow you to execute Python code directly.\n",
        "\n",
        "You can specify that a command should be executed in the shell (bash on Unix systems or PowerShell on Windows) by using `!` e.g., `!echo \"hello world\"`.\n",
        "\n",
        "### Text cell\n",
        "\n",
        "Text cells allow you to insert text in Markdown format (see Markdown syntax) to provide information.\n",
        "\n",
        "## Saving changes\n",
        "\n",
        "Don't forget to save changes regularly with the shortcut CTRL + S or in File > Save in case of a crash.\"\n"
      ],
      "metadata": {
        "id": "awMNZq6f8DQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation des bibliothèques"
      ],
      "metadata": {
        "id": "FWANILVpYg88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install Python libraries using the official Python library manager, pip.\n",
        "\n",
        "* Scikit-Learn for machine learning models\n",
        "* Matplotlib for creating graphs\n",
        "* Seaborn for simpler graphing\n",
        "* Pandas for CSV file manipulation (dataframes)"
      ],
      "metadata": {
        "id": "Ky-SaPvYIHar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYUR1pcI3riE"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Restart the kernel after installing the libraries:***\n",
        "\n",
        "(top toolbar) Kernel > Restart Kernel > Restart\n",
        "\n",
        "If you encounter any issues, save the notebook, close the page, and reopen the notebook.\n"
      ],
      "metadata": {
        "id": "_n15efun8IY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "8ly5GQ_gYl7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries and the dataset are imported\n",
        "\n"
      ],
      "metadata": {
        "id": "kbJ4zWMeZ8Wk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS0vWqTS4IKX"
      },
      "outputs": [],
      "source": [
        "# We'll work with breast cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***If it does not work, you must restart the kernel:***\n",
        "\n",
        "(top toolbar) Kernel > Restart Kernel > Restart"
      ],
      "metadata": {
        "id": "P9L6aXtuJMrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation of the dataset's content"
      ],
      "metadata": {
        "id": "3d4T0At9YqeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset and first handling"
      ],
      "metadata": {
        "id": "z2l0PouinPNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before preprocessing the dataset, it is recommended to perform a visualisation step. This helps in better understanding the dataset and knowing the types of variables for the features (quantitative, qualitative, etc.)."
      ],
      "metadata": {
        "id": "25JatIs2lrdA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hny_l_ni9qgT"
      },
      "source": [
        "We have two classes: B and M. These classes are contained in the **target** column (initially binary). Each class represents a cancer classification, and the codes are as follows:\n",
        "* 1 = \"benign\"\n",
        "* 0 = \"malignant\"\n",
        "\n",
        "We extract:\n",
        "* the features that will allow us to classify the instances\n",
        "* the labels containing the index and class columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3erq3PC4U-9"
      },
      "outputs": [],
      "source": [
        "# Load data as tuple\n",
        "features, labels = load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Concatenate features and labels together\n",
        "dataset = pd.concat([features, labels], axis=1)\n",
        "\n",
        "# Display the dataset's first lines\n",
        "display(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `target` column (0, 1) is changed into `target_names` containing strings (\"malign\", \"benign\")"
      ],
      "metadata": {
        "id": "nDCT3m8SQ9W3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFKfWnSBHY-D"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset from Sklearn library\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Creating a Pandas dataframe with column names corresponding to features\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add a column 'target_names' and replace binary data\n",
        "df['target_names'] = data.target_names[data.target]\n",
        "\n",
        "# Display the dataset's first lines\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All features are listed"
      ],
      "metadata": {
        "id": "V-2tLk0EtOq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display features' names of the dataset\n",
        "display(dataset.columns)\n",
        "\n",
        "# Create a list of columns containing the word \"mean\"\n",
        "mean_columns = [col for col in dataset.columns if 'mean' in col]\n",
        "\n",
        "# Create a list of columns containing the word \"error\"\n",
        "error_columns = [col for col in dataset.columns if 'error' in col]\n",
        "\n",
        "# Create a list of columns containing the word \"worst\"\n",
        "worst_columns = [col for col in dataset.columns if 'worst' in col]\n",
        "\n",
        "# Display the columns containing the word \"mean\"\n",
        "display(mean_columns)\n",
        "\n",
        "# Display the columns containing the word \"error\"\n",
        "display(error_columns)\n",
        "\n",
        "# Display the columns containing the word \"worst\"\n",
        "display(worst_columns)"
      ],
      "metadata": {
        "id": "MzOG0A1DXjRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 10 features: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension\n",
        "\n",
        "Each feature has:\n",
        "* an average (mean)\n",
        "* a standard deviation (error)\n",
        "* the mean of the 3 highest values (worst)"
      ],
      "metadata": {
        "id": "JAorOI2FySn9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IHTBKVE48p1"
      },
      "outputs": [],
      "source": [
        "def data_info(dataset: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Sums up the content of the dataset\n",
        "    \"\"\"\n",
        "    # Info about the dataset's dimensions\n",
        "    print(\"DIMENSIONS n\")\n",
        "    dims = dataset.shape\n",
        "    print(\"\\tThe dataset contains {} instances (observations), and {} features.\\n\".format(dims[0], dims[1]))\n",
        "    print(\"\\tThe features are: {}.\\n\".format(dataset.columns.tolist()))\n",
        "\n",
        "    # Info about classes\n",
        "    print(\"CLASSES:\\n\")\n",
        "    classes = df[\"target_names\"].value_counts().index.tolist()\n",
        "    print(\"\\tThere are {} classes in the dataset.\\n\".format(len(classes)))\n",
        "    print(\"\\tClasse are: {}.\\n\".format(classes))\n",
        "\n",
        "    # Info about duplicates\n",
        "    print(\"DUPLICATE:\\n\")\n",
        "    subset_columns = list(dataset.columns)\n",
        "    # Checking duplicates in the columns of the subset\n",
        "    duplicated_rows = dataset.duplicated(subset=subset_columns, keep=False)\n",
        "    # Couting duplicates\n",
        "    duplicated_count = duplicated_rows.sum()\n",
        "    print(\"\\tThere are {} duplicate(s) in the dataset.\\n\".format(duplicated_count))\n",
        "    # Display the number of duplicates\n",
        "    if duplicated_count > 0:\n",
        "      rows_duplicated = dataset[duplicated_rows]\n",
        "      print(\"\\tDuplicates are:\\n\")\n",
        "      print(rows_duplicated)\n",
        "    else:\n",
        "      print(\"\\tNo duplicate in the dataset.\\n\")\n",
        "\n",
        "data_info(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global statistics of the features"
      ],
      "metadata": {
        "id": "SdYGhZTqm_Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at stats per class.\n"
      ],
      "metadata": {
        "id": "4xtYEwSjt1bT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI2bdSZC67IZ"
      },
      "outputs": [],
      "source": [
        "def stats_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Global stats\n",
        "    print(\"GLOBAL STATS: \")\n",
        "    display(dataset.iloc[:, 0:30].describe())\n",
        "\n",
        "    # Stats per class\n",
        "    print(\"\\nSTATS PER CLASS\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist() :\n",
        "\n",
        "        print(\"\\n\\tClass: {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i].iloc[:, 0:30].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the features of the benign class (1) tend to be lower than those of the malignant class (0).\n",
        "\n",
        "For example:\n",
        "* The mean of `mean radius` is 12.146524 for the benign class (0) but 17.462830 for the malignant class (1).\n",
        "* The median of `mean texture` is 17.390000 for the benign class (0) and 21.460000 for the malignant class (1).\""
      ],
      "metadata": {
        "id": "6vzdQiwdwldk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the table is partially truncated, we are going to show features belonging to the `error` group, which corresponds to the standard deviation."
      ],
      "metadata": {
        "id": "_etzUQMAx-WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Display global stats for columns of the error group\n",
        "    \"\"\"\n",
        "    # Global stats\n",
        "    print(\"GLOBAL STATS: \")\n",
        "    display(dataset[error_columns].describe())\n",
        "\n",
        "    # Stats per class\n",
        "    print(\"\\nSTATS PER CLASS\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClass: {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][error_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "8xc-WqERxKi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1"
      ],
      "metadata": {
        "id": "fYl3UIpywVvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the most relevant group of features among `error`, `mean`, `worst`. Then display the global stats and stats per class of the chosen group.\n",
        "\n",
        "Use the variables `mean_columns`, `error_columns` or `worst_columns`."
      ],
      "metadata": {
        "id": "pjvnrbDKjlb1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bz-HCRP5XzGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box plots of the features"
      ],
      "metadata": {
        "id": "HnY3Fbogy0ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more visual way to show the distribution of the dataset is to use box plots, which allow us to observe trends between classes.\n",
        "\n",
        "Below, we are only looking at features belonging to the `error` group."
      ],
      "metadata": {
        "id": "HmkIs4LemYxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boxplot_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Retrieve classes\n",
        "    classes = dataset[\"target_names\"].value_counts().index.tolist()\n",
        "\n",
        "    # Retrieve variable names\n",
        "    features_names = [col_name for col_name in dataset.columns.tolist() if \"error\" in col_name and col_name != \"target\"]\n",
        "\n",
        "    # Define the subplot counter\n",
        "    cpt = 1\n",
        "\n",
        "    # Define a plot\n",
        "    plt.figure(figsize=(20, 40))\n",
        "\n",
        "    # For each variable\n",
        "    for col_name in features_names :\n",
        "        # On a subplot\n",
        "        plt.subplot(len(features_names), len(classes), cpt)\n",
        "\n",
        "        # Display the box plot of the values for each class\n",
        "        sns.boxplot(data=dataset, x=\"target_names\", y=col_name, hue=\"target_names\")\n",
        "\n",
        "        # Next subplot\n",
        "        cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "boxplot_data(df)"
      ],
      "metadata": {
        "id": "4aJng7199UI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2"
      ],
      "metadata": {
        "id": "aJFqbvBs9WQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same as above but select the feature group corresponding to `mean` and/or `worst` to visualise its box plots."
      ],
      "metadata": {
        "id": "SQ_u8OtS0PrY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8AcbivT11Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These box plots confirm our previous observations: instances of the malignant class tend to have lower feature values than instances of the benign class."
      ],
      "metadata": {
        "id": "-OJVIfRWmEfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scatter plots of the features"
      ],
      "metadata": {
        "id": "a3JUvVDY9KSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These scatterplots will allow us to show pairwise relationships between data variables. Each subplot in the grid represents the relationship between two different variables."
      ],
      "metadata": {
        "id": "BMO9U9MQlkoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsuZWQO2Llwm"
      },
      "outputs": [],
      "source": [
        "def features_distributions(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Retrive variable names\n",
        "    features_names = [\"mean perimeter\", \"mean texture\", \"mean area\", \"mean radius\"]\n",
        "\n",
        "    # Retrieve variable count\n",
        "    n_features = len(features_names)\n",
        "\n",
        "    # Define counter for the subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Define a plot\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    # Display each variable depending on the other\n",
        "    for i in features_names :\n",
        "\n",
        "        for j in features_names :\n",
        "\n",
        "            # On a subplot\n",
        "            plt.subplot(n_features, n_features, cpt)\n",
        "\n",
        "            # Display distribution of classes in the plan\n",
        "            plt.scatter(dataset.loc[:, i], dataset.loc[:, j], c=dataset[\"target\"])\n",
        "\n",
        "            # Name axes\n",
        "            plt.ylabel(i)\n",
        "            plt.xlabel(j)\n",
        "\n",
        "            # Next subplot\n",
        "            cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Display plot\n",
        "    plt.show()\n",
        "\n",
        "features_distributions(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these scatter plots, we can observe that instances of the same class (= same color) tend to cluster together. This is a positive sign for a classification approach."
      ],
      "metadata": {
        "id": "nCNMWEQIlHEW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jdAngBUTw_M"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data preprocessing step is crucial to prepare the dataset for training as models are sensitive to certain characteristics such as missing or duplicate data, outliers, etc. This step aims to homogenise the dataset.\n",
        "\n",
        "This preprocessing step includes several sub-steps:\n",
        "* **Cleaning**: removing duplicate/missing data\n",
        "* **Normalisation**: standardizing the data\n",
        "\n",
        "> Other sub-steps such as **feature extraction** (not presented here) involve extracting meaningful data. For example, in analyzing text, one might extract semantically rich words (tokens) and transform them into quantitative data. This way, textual information is converted into numbers.\n",
        "> Another sub-step is class **balancing**, which is not performed here. Some algorithms are less sensitive to imbalanced classes.\n",
        "> In connection with class balancing, a **resampling** sub-step may be necessary, involving reshaping the dataset to make it more balanced. Resampling includes downsampling, upsampling, and interpolation.\n",
        "\n",
        "Finally, the last step is **splitting** the dataset: creating separate training and validation datasets.\""
      ],
      "metadata": {
        "id": "qEMKlb9HFgnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOaq7ZkjTyRL"
      },
      "outputs": [],
      "source": [
        "# This function normalises data\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCedoZP5T2L9"
      },
      "outputs": [],
      "source": [
        "# Remove duplicates\n",
        "dataset = dataset[dataset.duplicated() == False].reset_index(drop=True)\n",
        "\n",
        "# Display info\n",
        "display(dataset.head())\n",
        "print(\"dimensions : {}\".format(dataset.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLc8EhjtT6E8"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "features, labels, target_names = dataset.iloc[:, 0:30], dataset.iloc[:, 30], df[\"target_names\"]\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "normalizer = MinMaxScaler()\n",
        "\n",
        "# Normalisation\n",
        "features = normalizer.fit_transform(features)\n",
        "\n",
        "# Transform into a dataframe\n",
        "features = pd.DataFrame(data=features, columns=normalizer.get_feature_names_out())\n",
        "\n",
        "# Display the results of normalisation\n",
        "display(features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset is relatively clean, there is no need for extensive preprocessing. We can proceed to the training phase."
      ],
      "metadata": {
        "id": "WbUiBGJYJj-F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwBiZwq6UFK5"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the name suggests, the training step involves training a model, meaning the model will adjust its parameters/weights to best predict the output: the class (malignant or benign)\n",
        "\n",
        "We're going to use classifiers\n",
        "* Naive Bayes\n",
        "* Logistic regression\n",
        "* Support-vector machine\n",
        "* Random forest\n",
        "* XGboost\n",
        "* Multilayer perceptron"
      ],
      "metadata": {
        "id": "CKZgD2jIKKLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDPNJiXHUF5A"
      },
      "outputs": [],
      "source": [
        "# Allow to split a dataset into a training and test dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# All the models we're going to use\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# Allow to evaluate models\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Find best hyperparameters\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxXo3eTBUJBi"
      },
      "outputs": [],
      "source": [
        "# Split data into 4 parts\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "# Display the sets\n",
        "## Input\n",
        "display(x_train.head())\n",
        "display(x_test.head())\n",
        "## Output: class to predict\n",
        "display(y_train.head())\n",
        "display(y_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training dataset corresponds to 80% of the original dataset size, and the test dataset corresponds to the remaining 20%.\n",
        "\n",
        "The splitting is done randomly, but we can 'fix the level of randomness' for reproducibility using the `random_state` parameter."
      ],
      "metadata": {
        "id": "oAWewGaPNLv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will evaluate the model's performance using a confusion matrix. A confusion matrix compares the model's predicted classes to the actual classes (true positive, false negative, etc.). From this matrix, performance metrics can be calculated:\n",
        "* Precision: The higher it is, the more true positives are maximised.\n",
        "* Recall: The higher it is, the more true (true positive and true negative) instances are maximised.\n",
        "* Accuracy: The higher it is, the more overall correct predictions (false negative and true positive) are maximised.\n",
        "* F1-score: It combines precision and recall (harmonic mean).\n",
        "\n",
        "All these metrics are important, but it is especially **precision** and **recall** (= sensitivity) that are of interest since favoring one often comes at the cost of the other (trade-off). Depending on the dataset and the classification objective, one might prefer to emphasise either precision or recall.\n",
        "\n",
        "For breast cancer prediction, the preference is often towards detecting more true positives: if a malignant tumor is detected, it is likely to be malignant. Therefore, there is a tendency to favor precision, given the severity of treatments. However, in the end, it remains a choice.\""
      ],
      "metadata": {
        "id": "en_FyBbPTdzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flhb5Jnf_piM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Define a list of models\n",
        "learning_algo = [MultinomialNB(),\n",
        "                 LogisticRegression(random_state=42),\n",
        "                 SVC(random_state=42),\n",
        "                 DecisionTreeClassifier(random_state=42),\n",
        "                 GradientBoostingClassifier(random_state=42),\n",
        "                 MLPClassifier(random_state=42, max_iter=1000)\n",
        "                ]\n",
        "\n",
        "# Initiliase variables to follow the best models and their metrics\n",
        "max_accuracy = 0\n",
        "max_precision = 0\n",
        "max_recall = 0\n",
        "best_model_accuracy = None\n",
        "best_model_precision = None\n",
        "best_model_recall = None\n",
        "\n",
        "# Loop through each algorithm\n",
        "for algo in learning_algo:\n",
        "    tmp = algo  # Assign the current algorithm to the tmp variable\n",
        "    print(type(tmp).__name__ + \":\")  # Display the name of the algorithm\n",
        "\n",
        "    # Training the model\n",
        "    tmp.fit(x_train, y_train)\n",
        "\n",
        "    # Create a plot to display the confusion matrix\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Display the confusion matrix for the training set\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_train, y_train, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 1))\n",
        "    plt.title(\"Confusion matrix - Training set\")\n",
        "\n",
        "    # Display the confusion matrix for the test set\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_test, y_test, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 2))\n",
        "    plt.title(\"Confusion matrix - Test set\")\n",
        "\n",
        "    plt.show()  # Display the confusion matrix\n",
        "\n",
        "    # Predict the training and test sets\n",
        "    y_pred_train = tmp.predict(x_train)\n",
        "    y_pred_test = tmp.predict(x_test)\n",
        "\n",
        "    # Compute the performance matrix for the training set\n",
        "    accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
        "    precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
        "    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
        "\n",
        "    # Compute the performance matrix for the test set\n",
        "    accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
        "    precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Display the classification report for the training set\n",
        "    print(\"\\nTRAINING SET :\\n\")\n",
        "    print(classification_report(y_pred_train, y_train))\n",
        "\n",
        "    # Display the performance metrics for the training set\n",
        "    print(\"\\nAccuracy of the training set:\", accuracy_train)\n",
        "    print(\"Precision of the training set:\", precision_train)\n",
        "    print(\"Recall of the training set:\", recall_train)\n",
        "\n",
        "    # Display the classifion repport for the test set\n",
        "    print(\"\\nTEST SET:\\n\")\n",
        "    print(classification_report(y_pred_test, y_test))\n",
        "\n",
        "    # Display the performance metrics for the test set\n",
        "    print(\"Accuracy of the test set\", accuracy_test)\n",
        "    print(\"Precision of the test set:\", precision_test)\n",
        "    print(\"Recall of the test set:\", recall_test)\n",
        "\n",
        "    # Update the max values and best model if the current model is the best one\n",
        "    if accuracy_test > max_accuracy:\n",
        "        max_accuracy = accuracy_test\n",
        "        best_model_accuracy = type(tmp).__name__\n",
        "\n",
        "    if precision_test > max_precision:\n",
        "        max_precision = precision_test\n",
        "        best_model_precision = type(tmp).__name__\n",
        "\n",
        "    if recall_test > max_recall:\n",
        "        max_recall = recall_test\n",
        "        best_model_recall = type(tmp).__name__\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "# Display the best models and their performance metrics\n",
        "print(f\"Best model (accuracy): {best_model_accuracy} - accuracy: {max_accuracy:.4f}\")\n",
        "print(f\"Best model (precision): {best_model_precision} - Precision: {max_precision:.4f}\")\n",
        "print(f\"Best model (recall): {best_model_recall} - recall: {max_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsequently, we will search for the best hyperparameters, meaning those that will yield the best predictions. Hyperparameters are specific parameters of algorithms that influence their behavior.\n",
        "\n",
        "Here, we are focused on optimising hyperparameters for all models:"
      ],
      "metadata": {
        "id": "DhyUtp05fEkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHjoik9mUnkZ"
      },
      "outputs": [],
      "source": [
        "# Instantiate a SVM\n",
        "model = SVC()\n",
        "\n",
        "# Define the parameters to test\n",
        "params = {\"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "          \"random_state\" : [i for i in range(0, 100, 1)]}\n",
        "\n",
        "# Instantiate the iterator for the creation of the validation set during the research of the best hyperparameters\n",
        "kfold = 10\n",
        "\n",
        "# Create a reseach grid\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", cv=kfold, verbose=3)\n",
        "\n",
        "# Research + display the result\n",
        "grid_search.fit(x_train, y_train)\n",
        "print(\"\\nThe best parameters for the models are: {}\".format(grid_search.best_params_))\n",
        "\n",
        "#The best parameters for the models are : {'C': 10, 'random_state': 0} with kfold= LeaveOneOut() et kfold= 10, to save time we use kfold= 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPXTVCpqUrjU"
      },
      "outputs": [],
      "source": [
        "# Retrieve the best model\n",
        "model = grid_search.best_estimator_\n",
        "print(model)\n",
        "# Create a plot\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Display the confusion matrix for the training set\n",
        "ConfusionMatrixDisplay.from_estimator(model, x_train, y_train, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 1))\n",
        "plt.title(\"Confusion matrix - Training Set\")\n",
        "\n",
        "# Display the confusion matrix for the test set\n",
        "ConfusionMatrixDisplay.from_estimator(model, x_test, y_test, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 2))\n",
        "plt.title(\"Confusion matrix - Test Set\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Display the classification report of the training set\n",
        "print(\"\\nTRAINING SET:\\n\")\n",
        "print(classification_report(model.predict(x_train), y_train))\n",
        "\n",
        "# Display the classification report of the test set\n",
        "print(\"\\nTEST SET :\\n\")\n",
        "print(classification_report(model.predict(x_test), y_test))\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLVDjSEKUvyO"
      },
      "source": [
        "# Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is saved to apply it on other datasets.\n",
        "\n",
        "The model is saved using the binary `.sav` format."
      ],
      "metadata": {
        "id": "jKmQ0tmzQSUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQE7NpT_UxDP"
      },
      "outputs": [],
      "source": [
        "# Allow to save Python objects\n",
        "import pickle as pk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtMDunldUzem"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model with the best hyperparameters\n",
        "model = SVC(C=10, random_state=0)\n",
        "\n",
        "# Train the model on the whole data\n",
        "model.fit(features, labels)\n",
        "\n",
        "# Display the confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.title(\"Final Confusion Matrix\")\n",
        "ConfusionMatrixDisplay.from_estimator(model, features, labels, cmap=plt.cm.Blues, ax=plt.subplot(1, 1, 1))\n",
        "plt.show()\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(classification_report(model.predict(features), labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoILiz4aU08g"
      },
      "outputs": [],
      "source": [
        "# Save the preprocessor, it'll come in handy for future data\n",
        "pk.dump(normalizer, open(\"preprocesseur.sav\", 'wb'))\n",
        "\n",
        "# Save the model\n",
        "pk.dump(model, open(\"model.sav\", 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic exercise"
      ],
      "metadata": {
        "id": "XT8PTRUJrSxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on what you've seen, train a model on a mini-dataset (see the cell below). Identify the important functions and train one or more of the six models mentioned earlier.\n",
        "\n",
        "In this mini-dataset, you need to predict the `y` class (dog or cat) from `x` (containing 2 features: height in cm and weight in kg). There are 6 instances."
      ],
      "metadata": {
        "id": "o3ep7f5zrYx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[45, 11.34], [56, 13.61], [51, 12.7], [30, 4.5], [25, 3.8], [35, 5.2]] # features\n",
        "y = [\"dog\", \"dog\", \"dog\", \"cat\", \"cat\", \"cat\"] # class"
      ],
      "metadata": {
        "id": "p_kS1cjksCK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corrections"
      ],
      "metadata": {
        "id": "QPn5x7yHoj5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1"
      ],
      "metadata": {
        "id": "jbfuNdGboozU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the group \"worst\"\n",
        "\n",
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    # Statistiques globales pour les colonnes contenant \"worst\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[worst_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"worst\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][worst_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "xg95TxLFYQEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the group \"error\"\n",
        "\n",
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    # Statistiques globales pour les colonnes contenant \"error\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[error_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"error\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][error_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "9FHT3hhho1wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the group \"mean\"\n",
        "\n",
        "def stats_data(dataset: pd.DataFrame) -> None:\n",
        "    # Statistiques globales pour les colonnes contenant \"mean\"\n",
        "    print(\"STATISTIQUES GLOBALES : \")\n",
        "    display(dataset[mean_columns].describe())\n",
        "\n",
        "    # Statistiques par classe pour les colonnes contenant \"mean\"\n",
        "    print(\"\\nSTATISTIQUES PAR CLASSE\")\n",
        "    for i in dataset[\"target\"].value_counts().index.tolist():\n",
        "        print(\"\\n\\tClasse : {}\".format(i))\n",
        "        display(dataset[dataset[\"target\"] == i][mean_columns].describe())\n",
        "\n",
        "\n",
        "stats_data(dataset)"
      ],
      "metadata": {
        "id": "fd6_8G9PpCmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2"
      ],
      "metadata": {
        "id": "PdfGQoJ4qV-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the group \"worst\"\n",
        "\n",
        "def boxplot_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Récupèrer les classes\n",
        "    classes = dataset[\"target_names\"].value_counts().index.tolist()\n",
        "\n",
        "    # Récupèrer le nom des variables\n",
        "    features_names = [col_name for col_name in dataset.columns.tolist() if \"worst\" in col_name and col_name != \"target\"] # Solution: changer le contenu de \"\" après le if\n",
        "\n",
        "    # Définir le compteur des subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Définir une figure\n",
        "    plt.figure(figsize=(20, 40))\n",
        "\n",
        "    # Pour chaque variables\n",
        "    for col_name in features_names :\n",
        "        # Sur un suplot\n",
        "        plt.subplot(len(features_names), len(classes), cpt)\n",
        "\n",
        "        # Afficher le boxplot de la distribution des valeurs de la variable, en fonction des espèces\n",
        "        sns.boxplot(data=dataset, x=\"target_names\", y=col_name, hue=\"target_names\")\n",
        "\n",
        "        # Passer au subplot suivant\n",
        "        cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Afficher la figure\n",
        "    plt.show()\n",
        "\n",
        "boxplot_data(df)"
      ],
      "metadata": {
        "id": "AY1MGe0aYeGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the group \"mean\"\n",
        "\n",
        "def boxplot_data(dataset: pd.DataFrame) -> None :\n",
        "\n",
        "    # Récupèrer les classes\n",
        "    classes = dataset[\"target_names\"].value_counts().index.tolist()\n",
        "\n",
        "    # Récupèrer le nom des variables\n",
        "    features_names = [col_name for col_name in dataset.columns.tolist() if \"mean\" in col_name and col_name != \"target\"] # Solution: changer le contenu de \"\" après le if\n",
        "\n",
        "    # Définir le compteur des subplots\n",
        "    cpt = 1\n",
        "\n",
        "    # Définir une figure\n",
        "    plt.figure(figsize=(20, 40))\n",
        "\n",
        "    # Pour chaque variables\n",
        "    for col_name in features_names :\n",
        "        # Sur un suplot\n",
        "        plt.subplot(len(features_names), len(classes), cpt)\n",
        "\n",
        "        # Afficher le boxplot de la distribution des valeurs de la variable, en fonction des espèces\n",
        "        sns.boxplot(data=dataset, x=\"target_names\", y=col_name, hue=\"target_names\")\n",
        "\n",
        "        # Passer au subplot suivant\n",
        "        cpt+=1\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "    # Afficher la figure\n",
        "    plt.show()\n",
        "\n",
        "boxplot_data(df)"
      ],
      "metadata": {
        "id": "Gj0l3xq-1w05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic exercise"
      ],
      "metadata": {
        "id": "1gjDAZZ-u52T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[45, 11.34], [56, 13.61], [51, 12.7], [30, 4.5], [25, 3.8], [35, 5.2]] # features\n",
        "y = [\"dog\", \"dog\", \"dog\", \"cat\", \"cat\", \"cat\"] # class\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, stratify=y, random_state=42)\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Define a list of models\n",
        "learning_algo = [MultinomialNB(),\n",
        "                 LogisticRegression(random_state=42),\n",
        "                 SVC(random_state=42),\n",
        "                 DecisionTreeClassifier(random_state=42),\n",
        "                 GradientBoostingClassifier(random_state=42),\n",
        "                 MLPClassifier(random_state=42, max_iter=1000)\n",
        "                ]\n",
        "\n",
        "# Initiliase variables to follow the best models and their metrics\n",
        "max_accuracy = 0\n",
        "max_precision = 0\n",
        "max_recall = 0\n",
        "best_model_accuracy = None\n",
        "best_model_precision = None\n",
        "best_model_recall = None\n",
        "\n",
        "# Loop through each algorithm\n",
        "for algo in learning_algo:\n",
        "    tmp = algo  # Assign the current algorithm to the tmp variable\n",
        "    print(type(tmp).__name__ + \":\")  # Display the name of the algorithm\n",
        "\n",
        "    # Training the model\n",
        "    tmp.fit(x_train, y_train)\n",
        "\n",
        "    # Create a plot to display the confusion matrix\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Display the confusion matrix for the training set\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_train, y_train, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 1))\n",
        "    plt.title(\"Confusion matrix - Training set\")\n",
        "\n",
        "    # Display the confusion matrix for the test set\n",
        "    ConfusionMatrixDisplay.from_estimator(tmp, x_test, y_test, cmap=plt.cm.Blues, ax=plt.subplot(2, 2, 2))\n",
        "    plt.title(\"Confusion matrix - Test set\")\n",
        "\n",
        "    plt.show()  # Display the confusion matrix\n",
        "\n",
        "    # Predict the training and test sets\n",
        "    y_pred_train = tmp.predict(x_train)\n",
        "    y_pred_test = tmp.predict(x_test)\n",
        "\n",
        "    # Compute the performance matrix for the training set\n",
        "    accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
        "    precision_train = precision_score(y_train, y_pred_train, average='weighted')\n",
        "    recall_train = recall_score(y_train, y_pred_train, average='weighted')\n",
        "\n",
        "    # Compute the performance matrix for the test set\n",
        "    accuracy_test = balanced_accuracy_score(y_test, y_pred_test)\n",
        "    precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Display the classification report for the training set\n",
        "    print(\"\\nTRAINING SET :\\n\")\n",
        "    print(classification_report(y_pred_train, y_train))\n",
        "\n",
        "    # Display the performance metrics for the training set\n",
        "    print(\"\\nAccuracy of the training set:\", accuracy_train)\n",
        "    print(\"Precision of the training set:\", precision_train)\n",
        "    print(\"Recall of the training set:\", recall_train)\n",
        "\n",
        "    # Display the classifion repport for the test set\n",
        "    print(\"\\nTEST SET:\\n\")\n",
        "    print(classification_report(y_pred_test, y_test))\n",
        "\n",
        "    # Display the performance metrics for the test set\n",
        "    print(\"Accuracy of the test set\", accuracy_test)\n",
        "    print(\"Precision of the test set:\", precision_test)\n",
        "    print(\"Recall of the test set:\", recall_test)\n",
        "\n",
        "    # Update the max values and best model if the current model is the best one\n",
        "    if accuracy_test > max_accuracy:\n",
        "        max_accuracy = accuracy_test\n",
        "        best_model_accuracy = type(tmp).__name__\n",
        "\n",
        "    if precision_test > max_precision:\n",
        "        max_precision = precision_test\n",
        "        best_model_precision = type(tmp).__name__\n",
        "\n",
        "    if recall_test > max_recall:\n",
        "        max_recall = recall_test\n",
        "        best_model_recall = type(tmp).__name__\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "# Display the best models and their performance metrics\n",
        "print(f\"Best model (accuracy): {best_model_accuracy} - accuracy: {max_accuracy:.4f}\")\n",
        "print(f\"Best model (precision): {best_model_precision} - Precision: {max_precision:.4f}\")\n",
        "print(f\"Best model (recall): {best_model_recall} - recall: {max_recall:.4f}\")"
      ],
      "metadata": {
        "id": "coOjdSVCu9MI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}